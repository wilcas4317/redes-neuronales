{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neurona Artificial Simple desde Cero (Descenso del Gradiente \"a pedal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Presentado por:**\n",
        "*   [Nombre del Profesor/Presentador]\n",
        "*   [Institución/Curso]\n",
        "*   [Fecha]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instalación de librerías necesarias (si no están presentes en el entorno de Colab)\n",
        "!pip install pandas openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Cargar el archivo de datos\n",
        "try:\n",
        "    df = pd.read_excel('datos_sesion3.xlsx')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: El archivo 'datos_sesion3.xlsx' no fue encontrado. Asegúrate de que esté en el directorio correcto.\")\n",
        "    # Crear un DataFrame vacío para evitar errores posteriores si el archivo no se encuentra\n",
        "    df = pd.DataFrame(columns=['frente', 'profundidad', 'precio'])\n",
        "\n",
        "\n",
        "# Extraer las series de datos\n",
        "# Asegurarse de que las columnas existen antes de intentar acceder a ellas\n",
        "frentes = df['frente'] if 'frente' in df.columns else pd.Series(dtype='float64')\n",
        "profundidades = df['profundidad'] if 'profundidad' in df.columns else pd.Series(dtype='float64')\n",
        "precios = df['precio'] if 'precio' in df.columns else pd.Series(dtype='float64')\n",
        "\n",
        "# Mostrar las primeras filas del DataFrame y las series para verificar\n",
        "print(\"DataFrame cargado:\")\n",
        "print(df.head())\n",
        "print(\"\\nFrentes:\")\n",
        "print(frentes.head())\n",
        "print(\"\\nProfundidades:\")\n",
        "print(profundidades.head())\n",
        "print(\"\\nPrecios:\")\n",
        "print(precios.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estructura de la Neurona y Descenso del Gradiente\n",
        "\n",
        "Nuestra neurona artificial simple tomará dos entradas (frente y profundidad del terreno) y intentará predecir el precio. La estructura es:\n",
        "\n",
        "*   **Entradas (features):**\n",
        "    *   `x1`: frente del terreno\n",
        "    *   `x2`: profundidad del terreno\n",
        "*   **Pesos (weights):**\n",
        "    *   `w1`: peso asociado a `x1`\n",
        "    *   `w2`: peso asociado a `x2`\n",
        "*   **Sesgo (bias):**\n",
        "    *   `b`: término de sesgo independiente\n",
        "\n",
        "**Cálculo de la Predicción (Salida de la Neurona):**\n",
        "\n",
        "La predicción (`y_pred`) se calcula como una combinación lineal de las entradas y los pesos, más el sesgo:\n",
        "`y_pred = (x1 * w1) + (x2 * w2) + b`\n",
        "\n",
        "**Función de Costo (Error Cuadrático Medio - MSE):**\n",
        "\n",
        "Para medir qué tan bien está funcionando nuestra neurona, usamos la función de costo MSE. El objetivo es minimizar este costo. La fórmula, usando `m` como el número total de ejemplos de entrenamiento, es:\n",
        "`J(w1, w2, b) = (1 / (2 * m)) * Σ( (y_real_i - y_pred_i)^2 )`\n",
        "donde el sumatorio (Σ) es sobre todos los ejemplos de entrenamiento.\n",
        "\n",
        "**Descenso del Gradiente (Actualización de Pesos):**\n",
        "\n",
        "Para minimizar el costo, ajustamos `w1`, `w2`, y `b` iterativamente usando el descenso del gradiente. Esto implica calcular las derivadas parciales de la función de costo con respecto a cada parámetro:\n",
        "\n",
        "*   **Derivada con respecto a `w1`:**\n",
        "    `dJ/dw1 = (1 / m) * Σ( (y_pred_i - y_real_i) * x1_i )`\n",
        "    (Nota: A veces se usa `(y_real_i - y_pred_i) * (-x1_i) / m` que es equivalente)\n",
        "\n",
        "*   **Derivada con respecto a `w2`:**\n",
        "    `dJ/dw2 = (1 / m) * Σ( (y_pred_i - y_real_i) * x2_i )`\n",
        "    (Nota: A veces se usa `(y_real_i - y_pred_i) * (-x2_i) / m` que es equivalente)\n",
        "\n",
        "*   **Derivada con respecto a `b`:**\n",
        "    `dJ/db = (1 / m) * Σ( (y_pred_i - y_real_i) * 1 )`\n",
        "    (Nota: A veces se usa `(y_real_i - y_pred_i) * (-1) / m` que es equivalente)\n",
        "\n",
        "**Reglas de Actualización:**\n",
        "\n",
        "Los pesos y el sesgo se actualizan en cada iteración (época) usando una tasa de aprendizaje (`η` o `alpha`):\n",
        "`w1 = w1 - η * (dJ/dw1)`\n",
        "`w2 = w2 - η * (dJ/dw2)`\n",
        "`b = b - η * (dJ/db)`\n",
        "\n",
        "Este proceso se repite durante un número determinado de épocas, con el objetivo de que los pesos converjan a valores que minimicen el error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculo_neurona(frente_i, profundidad_i, w1, w2, b):\n",
        "    \"\"\"Calcula la salida de la neurona (predicción).\"\"\"\n",
        "    # y_pred = (x1 * w1) + (x2 * w2) + b\n",
        "    return (frente_i * w1) + (profundidad_i * w2) + b\n",
        "\n",
        "def funcion_costo_mse(frentes, profundidades, precios, w1, w2, b):\n",
        "    \"\"\"Calcula el Error Cuadrático Medio (MSE).\"\"\"\n",
        "    m = len(frentes)\n",
        "    if m == 0:\n",
        "        return 0\n",
        "    \n",
        "    total_error = 0\n",
        "    for i in range(m):\n",
        "        y_pred_i = calculo_neurona(frentes.iloc[i], profundidades.iloc[i], w1, w2, b)\n",
        "        y_real_i = precios.iloc[i]\n",
        "        total_error += (y_real_i - y_pred_i)**2\n",
        "        \n",
        "    mse = total_error / (2 * m)\n",
        "    return mse\n",
        "\n",
        "def derivada_w1(frentes, profundidades, precios, w1, w2, b):\n",
        "    \"\"\"Calcula la derivada parcial del costo MSE con respecto a w1.\"\"\"\n",
        "    m = len(frentes)\n",
        "    if m == 0:\n",
        "        return 0\n",
        "        \n",
        "    total_derivada_w1 = 0\n",
        "    for i in range(m):\n",
        "        y_pred_i = calculo_neurona(frentes.iloc[i], profundidades.iloc[i], w1, w2, b)\n",
        "        y_real_i = precios.iloc[i]\n",
        "        # (real - pred) * (-X_respectiva)\n",
        "        total_derivada_w1 += (y_real_i - y_pred_i) * (-frentes.iloc[i])\n",
        "        \n",
        "    return total_derivada_w1 / m\n",
        "\n",
        "def derivada_w2(frentes, profundidades, precios, w1, w2, b):\n",
        "    \"\"\"Calcula la derivada parcial del costo MSE con respecto a w2.\"\"\"\n",
        "    m = len(frentes)\n",
        "    if m == 0:\n",
        "        return 0\n",
        "        \n",
        "    total_derivada_w2 = 0\n",
        "    for i in range(m):\n",
        "        y_pred_i = calculo_neurona(frentes.iloc[i], profundidades.iloc[i], w1, w2, b)\n",
        "        y_real_i = precios.iloc[i]\n",
        "        # (real - pred) * (-X_respectiva)\n",
        "        total_derivada_w2 += (y_real_i - y_pred_i) * (-profundidades.iloc[i])\n",
        "        \n",
        "    return total_derivada_w2 / m\n",
        "\n",
        "def derivada_b(frentes, profundidades, precios, w1, w2, b):\n",
        "    \"\"\"Calcula la derivada parcial del costo MSE con respecto a b.\"\"\"\n",
        "    m = len(frentes)\n",
        "    if m == 0:\n",
        "        return 0\n",
        "        \n",
        "    total_derivada_b = 0\n",
        "    for i in range(m):\n",
        "        y_pred_i = calculo_neurona(frentes.iloc[i], profundidades.iloc[i], w1, w2, b)\n",
        "        y_real_i = precios.iloc[i]\n",
        "        # (real - pred) * (-1)\n",
        "        total_derivada_b += (y_real_i - y_pred_i) * (-1)\n",
        "        \n",
        "    return total_derivada_b / m\n",
        "\n",
        "# Prueba rápida de las funciones (opcional, se puede comentar o eliminar después)\n",
        "# print(\"Funciones definidas. Realizando una prueba rápida...\")\n",
        "# w1_test, w2_test, b_test = 0.1, 0.2, 0.05\n",
        "# if not frentes.empty and not profundidades.empty and not precios.empty:\n",
        "#     y_pred_test = calculo_neurona(frentes.iloc[0], profundidades.iloc[0], w1_test, w2_test, b_test)\n",
        "#     print(f\"Predicción para el primer dato con pesos de prueba: {y_pred_test}\")\n",
        "#     costo_test = funcion_costo_mse(frentes, profundidades, precios, w1_test, w2_test, b_test)\n",
        "#     print(f\"Costo MSE con pesos de prueba: {costo_test}\")\n",
        "#     dw1_test = derivada_w1(frentes, profundidades, precios, w1_test, w2_test, b_test)\n",
        "#     print(f\"Derivada w1 con pesos de prueba: {dw1_test}\")\n",
        "#     dw2_test = derivada_w2(frentes, profundidades, precios, w1_test, w2_test, b_test)\n",
        "#     print(f\"Derivada w2 con pesos de prueba: {dw2_test}\")\n",
        "#     db_test = derivada_b(frentes, profundidades, precios, w1_test, w2_test, b_test)\n",
        "#     print(f\"Derivada b con pesos de prueba: {db_test}\")\n",
        "# else:\n",
        "#     print(\"No hay datos cargados para realizar la prueba rápida de funciones.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def proceso_entrenamiento(frentes_train, profundidades_train, precios_train, tasa_aprendizaje_n, num_epocas):\n",
        "    \"\"\"\n",
        "    Implementa el descenso del gradiente para entrenar la neurona.\n",
        "    Inicializa los pesos w1, w2, b en cero.\n",
        "    Devuelve los pesos finales y una lista con el error MSE por época.\n",
        "    \"\"\"\n",
        "    w1 = 0.0\n",
        "    w2 = 0.0\n",
        "    b = 0.0\n",
        "    \n",
        "    historial_error_mse = []\n",
        "    \n",
        "    if frentes_train.empty or profundidades_train.empty or precios_train.empty:\n",
        "        print(\"Datos de entrenamiento vacíos. No se puede proceder con el entrenamiento.\")\n",
        "        return w1, w2, b, historial_error_mse\n",
        "\n",
        "    for epoca in range(num_epocas):\n",
        "        # Calcular derivadas\n",
        "        dj_dw1 = derivada_w1(frentes_train, profundidades_train, precios_train, w1, w2, b)\n",
        "        dj_dw2 = derivada_w2(frentes_train, profundidades_train, precios_train, w1, w2, b)\n",
        "        dj_db = derivada_b(frentes_train, profundidades_train, precios_train, w1, w2, b)\n",
        "        \n",
        "        # Actualizar pesos y sesgo\n",
        "        w1 = w1 - tasa_aprendizaje_n * dj_dw1\n",
        "        w2 = w2 - tasa_aprendizaje_n * dj_dw2\n",
        "        b = b - tasa_aprendizaje_n * dj_db\n",
        "        \n",
        "        # Calcular y registrar el costo MSE para esta época\n",
        "        costo_actual = funcion_costo_mse(frentes_train, profundidades_train, precios_train, w1, w2, b)\n",
        "        historial_error_mse.append(costo_actual)\n",
        "        \n",
        "    return w1, w2, b, historial_error_mse\n",
        "\n",
        "# Prueba de la función de entrenamiento (opcional)\n",
        "# print(\"Función de entrenamiento definida.\")\n",
        "# if not frentes.empty:\n",
        "#     tasa_prueba = 0.0001 # Puede necesitar ajuste según los datos\n",
        "#     epocas_prueba = 10\n",
        "#     print(f\"Iniciando prueba de entrenamiento con {epocas_prueba} épocas y tasa {tasa_prueba}...\")\n",
        "#     w1_final_prueba, w2_final_prueba, b_final_prueba, errores_prueba = proceso_entrenamiento(frentes, profundidades, precios, tasa_prueba, epocas_prueba)\n",
        "#     print(f\"Pesos finales de prueba: w1={w1_final_prueba}, w2={w2_final_prueba}, b={b_final_prueba}\")\n",
        "#     if errores_prueba:\n",
        "#         print(f\"Error MSE inicial (prueba): {errores_prueba[0]}\")\n",
        "#         print(f\"Error MSE final (prueba): {errores_prueba[-1]}\")\n",
        "# else:\n",
        "#     print(\"No hay datos cargados para probar la función de entrenamiento.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parámetros de entrenamiento\n",
        "# Estos valores pueden necesitar ajuste dependiendo de la escala de los datos y la convergencia observada.\n",
        "# Una tasa de aprendizaje muy grande puede hacer que el error diverja.\n",
        "# Una tasa muy pequeña puede hacer que el entrenamiento sea muy lento.\n",
        "# Normalizar los datos de entrada (frentes, profundidades, precios) puede ayudar a encontrar\n",
        "# una buena tasa de aprendizaje más fácilmente y mejorar la convergencia.\n",
        "# Por ahora, usaremos valores que podrían funcionar con datos no normalizados,\n",
        "# pero esto es altamente dependiente de la magnitud de los precios y las características.\n",
        "\n",
        "# Dada la magnitud típica de los precios (ej: 250000) y las características (ej: 10, 20),\n",
        "# el gradiente puede ser muy grande. (y_real - y_pred) * (-X).\n",
        "# Si (y_real - y_pred) es del orden de 10000 y X es 10, el gradiente es 100000.\n",
        "# Si la tasa es 0.01, el cambio de peso es 1000, lo cual es enorme.\n",
        "# Se necesita una tasa de aprendizaje MUY pequeña.\n",
        "\n",
        "# Intentemos con valores iniciales, pueden necesitarse ajustes:\n",
        "tasa_aprendizaje = 1e-7 # Ejemplo: 0.0000001. Si los precios son muy altos, podría ser incluso menor.\n",
        "                        # O considerar normalizar los datos.\n",
        "num_epocas_entrenamiento = 1000 # Empezar con 1000, se puede aumentar.\n",
        "\n",
        "print(f\"Iniciando entrenamiento con tasa de aprendizaje: {tasa_aprendizaje} y {num_epocas_entrenamiento} épocas.\")\n",
        "print(\"---\")\n",
        "print(\"Si los datos no han sido cargados (ej. 'frentes' está vacío), el entrenamiento no se ejecutará.\")\n",
        "print(\"Asegúrese de que 'datos_sesion3.xlsx' está presente y las celdas anteriores se han ejecutado.\")\n",
        "print(\"---\")\n",
        "\n",
        "# Asegurarse de que las series de datos no están vacías antes de entrenar\n",
        "if not frentes.empty and not profundidades.empty and not precios.empty:\n",
        "    w1_final, w2_final, b_final, historial_mse = proceso_entrenamiento(\n",
        "        frentes, \n",
        "        profundidades, \n",
        "        precios, \n",
        "        tasa_aprendizaje, \n",
        "        num_epocas_entrenamiento\n",
        "    )\n",
        "\n",
        "    print(f\"Entrenamiento completado.\")\n",
        "    print(f\"Pesos finales:\")\n",
        "    print(f\"  w1 (peso para 'frente'): {w1_final:.4f}\")\n",
        "    print(f\"  w2 (peso para 'profundidad'): {w2_final:.4f}\")\n",
        "    print(f\"  b (sesgo): {b_final:.4f}\")\n",
        "    print(\"\\n--- Evolución del Error MSE ---\")\n",
        "    if historial_mse:\n",
        "        print(f\"Error MSE inicial (Época 1): {historial_mse[0]:.2f}\")\n",
        "        \n",
        "        # Imprimir error cada 100 épocas\n",
        "        for i in range(0, len(historial_mse), 100):\n",
        "            if i == 0: continue # Ya se imprimió el inicial\n",
        "            if i < len(historial_mse):\n",
        "                 print(f\"Error MSE en Época {i+1}: {historial_mse[i]:.2f}\")\n",
        "        \n",
        "        print(f\"Error MSE final (Época {num_epocas_entrenamiento}): {historial_mse[-1]:.2f}\")\n",
        "    else:\n",
        "        print(\"No se generó historial de MSE (posiblemente datos vacíos).\")\n",
        "else:\n",
        "    print(\"Entrenamiento omitido porque los datos (frentes, profundidades, precios) no están cargados.\")\n",
        "    # Asignar valores por defecto para que el resto del notebook no falle si se ejecuta sin datos\n",
        "    w1_final, w2_final, b_final = 0,0,0\n",
        "    historial_mse = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if historial_mse:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, num_epocas_entrenamiento + 1), historial_mse)\n",
        "    plt.xlabel(\"Época\")\n",
        "    plt.ylabel(\"Error Cuadrático Medio (MSE)\")\n",
        "    plt.title(\"Evolución del Error MSE durante el Entrenamiento\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No hay historial de MSE para graficar. Asegúrese de que el entrenamiento se haya ejecutado con datos.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusiones de la Práctica\n",
        "\n",
        "Esta práctica nos ha permitido construir una neurona artificial desde sus componentes más básicos, implementando el algoritmo de descenso del gradiente \"a pedal\". A través de este ejercicio, hemos podido observar directamente:\n",
        "\n",
        "*   **El Flujo de Datos:** Cómo las entradas (frente, profundidad) se combinan con los pesos y el sesgo para generar una predicción.\n",
        "*   **La Importancia de la Función de Costo:** El MSE nos dio una medida cuantitativa de qué tan erradas estaban nuestras predicciones, guiando el aprendizaje.\n",
        "*   **El Mecanismo del Descenso del Gradiente:** Vimos cómo el cálculo de las derivadas nos indica la dirección para ajustar los pesos y el sesgo con el fin de minimizar el error. La tasa de aprendizaje juega un papel crucial aquí; una tasa inadecuada puede impedir la convergencia o ralentizarla excesivamente.\n",
        "*   **El Proceso Iterativo del Aprendizaje:** La repetición del cálculo de predicciones, errores, derivadas y actualizaciones de pesos a lo largo de múltiples épocas es fundamental para que la neurona \"aprenda\".\n",
        "*   **Visualización del Aprendizaje:** La gráfica de MSE vs. Épocas es una herramienta visual poderosa para entender si el modelo está aprendiendo (el error disminuye) o si hay problemas (el error se estanca, aumenta o fluctúa erráticamente).\n",
        "\n",
        "**Aprendizajes Clave:**\n",
        "\n",
        "*   **Sensibilidad a los Hiperparámetros:** La elección de la tasa de aprendizaje y el número de épocas es crítica. En este ejercicio, vimos que con datos no normalizados y valores de salida (precios) grandes, se requiere una tasa de aprendizaje muy pequeña para evitar que los ajustes de los pesos sean demasiado grandes y el error diverja.\n",
        "*   **Necesidad de Normalización (Potencial):** Aunque no se implementó aquí para mantener la simplicidad, en casos reales, normalizar las variables de entrada y/o salida puede hacer que el entrenamiento sea más estable y rápido, permitiendo el uso de tasas de aprendizaje más \"estándar\".\n",
        "*   **Fundamentos para Redes Neuronales Más Complejas:** Entender esta neurona simple es el primer paso para comprender redes neuronales más profundas y complejas. Los principios de propagación hacia adelante (cálculo de la predicción) y retropropagación del error (aunque aquí simplificada al cálculo directo de derivadas para el descenso del gradiente) son fundamentales.\n",
        "\n",
        "Este ejercicio práctico refuerza la comprensión teórica y demuestra cómo, con herramientas básicas de Python y un entendimiento de los conceptos matemáticos subyacentes, podemos implementar los bloques constructivos del aprendizaje automático."
      ]
    }
  ]
}
